Amazon
========================================================

Sentiment Analysis for Amazon.
```{r message=FALSE}
library("twitteR")
library("wordcloud")
library("tm")
library("ggplot2")
library("reshape2")
```
This code shows the twitter sentiment analysis using tweets of Amazon for 5 consecutive days. 
```{r eval=FALSE}
my.key <-"KbBDqQifBUi1HN5SKDABGt2VA"
my.secret <-"tnJUP5JTZQXQ9TFZP91JmKamad1sEBwR7NfuYcyAxnNbm8oW97"

cred <- OAuthFactory$new(consumerKey=my.key,
                         consumerSecret=my.secret,
                         requestURL='https://api.twitter.com/oauth/request_token',
                         accessURL='https://api.twitter.com/oauth/access_token',
                         authURL= 'https://api.twitter.com/oauth/authorize')

## save the credentials to your local drive
## on future uses of the script you'll only need to load the .Rdata
## file and won't have to re-authorize your account.

save(cred, file="twitter_authentication.Rdata")


load("twitter_authentication.Rdata")
cred$handshake(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl"))


## check that authorization was successful

registerTwitterOAuth(cred)


## tweets <- searchTwitter("@Boeing",n=500)

tweets <- searchTwitter("@Amazon",n=200, lang="en", since='2014-12-3', until='2014-12-4')

length(tweets)

## Now, we will extract the fields that we want

tweets.id <- sapply(tweets, function(x) x$getId())
tweets.text <- sapply(tweets, function(x) x$getText())
tweets.screenname <- sapply(tweets, function(x) x$getScreenName())
tweets.isretweet <- sapply(tweets, function(x) x$getIsRetweet())
tweets.retweeted <- sapply(tweets, function(x) x$getRetweeted())
tweets.created <- sapply(tweets, function(x) x$getCreated())

head(tweets.text)

## Write data to file
df <- data.frame(tweets.id, tweets.text, tweets.screenname, tweets.isretweet, tweets.retweeted, tweets.created)
names(df) <-c("id", "text", "screenname", "isretweet", "retweeted", "created")
write.table(df, file = "Amazon1.txt", append = TRUE)
## these are the files from ReggieNet
##load opinion lexicon
##from http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon
## the load path is relative to the working directory that is set above
pos <- scan("positive-words.txt",what="character",comment.char=";")
neg <- scan("negative-words.txt",what="character",comment.char=";")

## create corpus
## these functions from the tm package
tweets.corpus <- Corpus(VectorSource(tweets.text))

## clean up
tweets.corpus <- tm_map(tweets.corpus, tolower) 
tweets.corpus <- tm_map(tweets.corpus, removePunctuation)
tweets.corpus <- tm_map(tweets.corpus, function(x) removeWords(x,stopwords()))

## split the tweets in the corpus up into individual words
## lapply iterates over each element in the corpus
## and applies the strsplit function
## the splitting argument is the 3rd in the lapply function
## and is splitting on white space.
corpus.split <- lapply(tweets.corpus,strsplit,"\\s+")

## find matches for the individual words in the two lexicons
## lapply again, x takes on an element in the corpus
## at each iteration
matches <- lapply(corpus.split,function(x) {
  match.pos <- match(x[[1]],pos)
  match.neg <- match(x[[1]],neg) 
  
  ## return the length of each match vector, non-na 
  list(length(which(!is.na(match.pos))),length(which(!is.na(match.neg))))
})


## turn the matches into a matrix
## one column for positive, one for negative
match.matrix <- matrix(unlist(matches),nrow=length(matches),ncol=2,byrow=T)

## calculate a simple sentiment score by substracting
## positive count from negative count
simple.sentiment1 <- match.matrix[,1] - match.matrix[,2]

## histogram of sentiment
hist(simple.sentiment1)

sentiment1<-sum(simple.sentiment1)

  ###############################################################################
## 2nd Day Tweets


## tweets <- searchTwitter("@Boeing",n=500)

tweets <- searchTwitter("@Amazon",n=200, lang="en", since='2014-12-4', until='2014-12-5')

length(tweets)

tweets.id <- sapply(tweets, function(x) x$getId())
tweets.text <- sapply(tweets, function(x) x$getText())
tweets.screenname <- sapply(tweets, function(x) x$getScreenName())
tweets.isretweet <- sapply(tweets, function(x) x$getIsRetweet())
tweets.retweeted <- sapply(tweets, function(x) x$getRetweeted())
tweets.created <- sapply(tweets, function(x) x$getCreated())

head(tweets.text)

## Write data to file
df <- data.frame(tweets.id, tweets.text, tweets.screenname, tweets.isretweet, tweets.retweeted, tweets.created)
names(df) <-c("id", "text", "screenname", "isretweet", "retweeted", "created")

write.table(df, file = "Amazon2.txt", append = TRUE)

pos <- scan("positive-words.txt",what="character",comment.char=";")
neg <- scan("negative-words.txt",what="character",comment.char=";")

## create corpus
## these functions from the tm package
tweets.corpus <- Corpus(VectorSource(tweets.text))

## clean up
tweets.corpus <- tm_map(tweets.corpus, tolower) 
tweets.corpus <- tm_map(tweets.corpus, removePunctuation)
tweets.corpus <- tm_map(tweets.corpus, function(x) removeWords(x,stopwords()))

## split the tweets in the corpus up into individual words
## lapply iterates over each element in the corpus
## and applies the strsplit function
## the splitting argument is the 3rd in the lapply function
## and is splitting on white space.
corpus.split <- lapply(tweets.corpus,strsplit,"\\s+")

## find matches for the individual words in the two lexicons
## lapply again, x takes on an element in the corpus
## at each iteration
matches <- lapply(corpus.split,function(x) {
  match.pos <- match(x[[1]],pos)
  match.neg <- match(x[[1]],neg) 
  
  ## return the length of each match vector, non-na 
  list(length(which(!is.na(match.pos))),length(which(!is.na(match.neg))))
})


## turn the matches into a matrix
## one column for positive, one for negative
match.matrix <- matrix(unlist(matches),nrow=length(matches),ncol=2,byrow=T)

## calculate a simple sentiment score by substracting
## positive count from negative count
simple.sentiment2 <- match.matrix[,1] - match.matrix[,2]

## histogram of sentiment
hist(simple.sentiment2)


sentiment2<-sum(simple.sentiment2)

  ###############################################################################
## 3rd Day Tweets

tweets <- searchTwitter("@Amazon",n=200, lang="en", since='2014-12-5', until='2014-12-6')

length(tweets)


## Now, we will extract the fields that we want


tweets.id <- sapply(tweets, function(x) x$getId())
tweets.text <- sapply(tweets, function(x) x$getText())
tweets.screenname <- sapply(tweets, function(x) x$getScreenName())
tweets.isretweet <- sapply(tweets, function(x) x$getIsRetweet())
tweets.retweeted <- sapply(tweets, function(x) x$getRetweeted())
tweets.created <- sapply(tweets, function(x) x$getCreated())

head(tweets.text)

## Write data to file
df <- data.frame(tweets.id, tweets.text, tweets.screenname, tweets.isretweet, tweets.retweeted, tweets.created)
names(df) <-c("id", "text", "screenname", "isretweet", "retweeted", "created")

write.table(df, file = "Amazon3.txt", append = TRUE)

## these are the files from ReggieNet
##load opinion lexicon
##from http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon
## the load path is relative to the working directory that is set above
pos <- scan("positive-words.txt",what="character",comment.char=";")
neg <- scan("negative-words.txt",what="character",comment.char=";")

## create corpus
## these functions from the tm package
tweets.corpus <- Corpus(VectorSource(tweets.text))

## clean up
tweets.corpus <- tm_map(tweets.corpus, tolower) 
tweets.corpus <- tm_map(tweets.corpus, removePunctuation)
tweets.corpus <- tm_map(tweets.corpus, function(x) removeWords(x,stopwords()))

## split the tweets in the corpus up into individual words
## lapply iterates over each element in the corpus
## and applies the strsplit function
## the splitting argument is the 3rd in the lapply function
## and is splitting on white space.
corpus.split <- lapply(tweets.corpus,strsplit,"\\s+")

## find matches for the individual words in the two lexicons
## lapply again, x takes on an element in the corpus
## at each iteration
matches <- lapply(corpus.split,function(x) {
  match.pos <- match(x[[1]],pos)
  match.neg <- match(x[[1]],neg) 
  
  ## return the length of each match vector, non-na 
  list(length(which(!is.na(match.pos))),length(which(!is.na(match.neg))))
})

## turn the matches into a matrix
## one column for positive, one for negative
match.matrix <- matrix(unlist(matches),nrow=length(matches),ncol=2,byrow=T)

## calculate a simple sentiment score by substracting
## positive count from negative count
simple.sentiment3 <- match.matrix[,1] - match.matrix[,2]

## histogram of sentiment
hist(simple.sentiment3)

sentiment3<-sum(simple.sentiment3)

  ###############################################################################
## 4th Day Tweets

tweets <- searchTwitter("@Amazon",n=200, lang="en", since='2014-12-8', until='2014-12-9')


length(tweets)

## Now, we will extract the fields that we want
tweets.id <- sapply(tweets, function(x) x$getId())
tweets.text <- sapply(tweets, function(x) x$getText())
tweets.screenname <- sapply(tweets, function(x) x$getScreenName())
tweets.isretweet <- sapply(tweets, function(x) x$getIsRetweet())
tweets.retweeted <- sapply(tweets, function(x) x$getRetweeted())
tweets.created <- sapply(tweets, function(x) x$getCreated())

head(tweets.text)

## Write data to file
df <- data.frame(tweets.id, tweets.text, tweets.screenname, tweets.isretweet, tweets.retweeted, tweets.created)
names(df) <-c("id", "text", "screenname", "isretweet", "retweeted", "created")

write.table(df, file = "Amazon4.txt", append = TRUE)

## these are the files from ReggieNet
##load opinion lexicon
##from http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon
## the load path is relative to the working directory that is set above
pos <- scan("positive-words.txt",what="character",comment.char=";")
neg <- scan("negative-words.txt",what="character",comment.char=";")

## create corpus
## these functions from the tm package
tweets.corpus <- Corpus(VectorSource(tweets.text))


## clean up
tweets.corpus <- tm_map(tweets.corpus, tolower) 
tweets.corpus <- tm_map(tweets.corpus, removePunctuation)
tweets.corpus <- tm_map(tweets.corpus, function(x) removeWords(x,stopwords()))

## split the tweets in the corpus up into individual words
## lapply iterates over each element in the corpus
## and applies the strsplit function
## the splitting argument is the 3rd in the lapply function
## and is splitting on white space.
corpus.split <- lapply(tweets.corpus,strsplit,"\\s+")

## find matches for the individual words in the two lexicons
## lapply again, x takes on an element in the corpus
## at each iteration
matches <- lapply(corpus.split,function(x) {
  match.pos <- match(x[[1]],pos)
  match.neg <- match(x[[1]],neg) 
  
  ## return the length of each match vector, non-na 
  list(length(which(!is.na(match.pos))),length(which(!is.na(match.neg))))
})


## turn the matches into a matrix
## one column for positive, one for negative
match.matrix <- matrix(unlist(matches),nrow=length(matches),ncol=2,byrow=T)

## calculate a simple sentiment score by substracting
## positive count from negative count
simple.sentiment4 <- match.matrix[,1] - match.matrix[,2]

## histogram of sentiment
hist(simple.sentiment4)
sentiment4<-sum(simple.sentiment4)

  ###############################################################################
## 5th Day Tweets

tweets <- searchTwitter("@Amazon",n=200, lang="en", since='2014-12-9', until='2014-12-10')


length(tweets)

## Now, we will extract the fields that we want

tweets.id <- sapply(tweets, function(x) x$getId())
tweets.text <- sapply(tweets, function(x) x$getText())
tweets.screenname <- sapply(tweets, function(x) x$getScreenName())
tweets.isretweet <- sapply(tweets, function(x) x$getIsRetweet())
tweets.retweeted <- sapply(tweets, function(x) x$getRetweeted())
tweets.created <- sapply(tweets, function(x) x$getCreated())

head(tweets.text)

## Write data to file
df <- data.frame(tweets.id, tweets.text, tweets.screenname, tweets.isretweet, tweets.retweeted, tweets.created)
names(df) <-c("id", "text", "screenname", "isretweet", "retweeted", "created")

write.table(df, file = "Amazon5.txt", append = TRUE)

## these are the files from ReggieNet
##load opinion lexicon
##from http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon
## the load path is relative to the working directory that is set above
pos <- scan("positive-words.txt",what="character",comment.char=";")
neg <- scan("negative-words.txt",what="character",comment.char=";")

## create corpus
## these functions from the tm package
tweets.corpus <- Corpus(VectorSource(tweets.text))

## clean up
tweets.corpus <- tm_map(tweets.corpus, tolower) 
tweets.corpus <- tm_map(tweets.corpus, removePunctuation)
tweets.corpus <- tm_map(tweets.corpus, function(x) removeWords(x,stopwords()))

## split the tweets in the corpus up into individual words
## lapply iterates over each element in the corpus
## and applies the strsplit function
## the splitting argument is the 3rd in the lapply function
## and is splitting on white space.
corpus.split <- lapply(tweets.corpus,strsplit,"\\s+")

## find matches for the individual words in the two lexicons
## lapply again, x takes on an element in the corpus
## at each iteration
matches <- lapply(corpus.split,function(x) {
  match.pos <- match(x[[1]],pos)
  match.neg <- match(x[[1]],neg) 
  
  ## return the length of each match vector, non-na 
  list(length(which(!is.na(match.pos))),length(which(!is.na(match.neg))))
})


## turn the matches into a matrix
## one column for positive, one for negative
match.matrix <- matrix(unlist(matches),nrow=length(matches),ncol=2,byrow=T)

## calculate a simple sentiment score by substracting
## positive count from negative count
simple.sentiment5 <- match.matrix[,1] - match.matrix[,2]

## histogram of sentiment
hist(simple.sentiment5)

sentiment5 <- sum(simple.sentiment5)
```

Following is the graph which shows the comparison between sentiment analysis and actual Stock Market Price. 

```{r}
#Create CSV file and update the data from stocks for Amazon. 

AmazonData <- read.csv(file = "Amazon.csv")

AmazonData$Start <- NULL
AmazonData$End <- NULL

AmazonData

AmazonData$Stock.Price <- NULL
AmazonData$Sentiment.Score <- NULL

molten.AmazonData <- melt(AmazonData, id.vars = c("Day", "Tweets"), 
                           measure.vars = c("Sentiment.Prediction", "Actual"))
```

```{r fig.width=7, fig.height=6}

ggplot(molten.AmazonData, aes(x=Day, y=value, fill = variable))+geom_bar(stat="identity", position=position_dodge())+xlab("Day")+ylab("Value")+ ggtitle("Amazon Stock Sentiment Analysis")


```

Summary

Here we calculated the sentiment analysis for Amazon using twitter and compared the analysis with ctual Stock Price of the Market. After performing sentiment analysis using tweets from Twitter we received varied analysis for all 5 days. We received positive for the 1st day, positive for 2nd day, negative for the 3rd day, positive for the 4th day and negative for the 5th day. However, the stock price of Amazon said a different story. It increased on day2 and day 5. 

Based on the above theory and graph, we can conclude that there is no clear corelation between twitter sentiment analysis and the stock market price. The changes in the stock market are due to different reasons like supply and demand of the products. It is not based on twitter analysis. 

  